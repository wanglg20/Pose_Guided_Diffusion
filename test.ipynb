{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test for attentino block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\zero123\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset.dataset import *\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_ds = RealEstate_dataset(info_root = 'dataset/RealEstate10K', data_root = 'dataset/RealEstate_data', mode = 'test')\n",
    "img_s, img_t, K_s, R_rel, t_rel = test_ds[1]\n",
    "\n",
    "\n",
    "from models.epipolar_unet import *\n",
    "\n",
    "W_Mat = epipolar_weight_Mat((64, 64), K_s, R_rel, t_rel)\n",
    "\n",
    "#print(W_Mat.shape)\n",
    "\n",
    "attn_resolution = 32\n",
    "query_channels = 256\n",
    "key_channels = 1\n",
    "t = torch.tensor([1000])\n",
    "source_input = torch.randn(1, 1, 128, 128)\n",
    "Unet_input = torch.randn(1, query_channels, attn_resolution, attn_resolution)\n",
    "\n",
    "e_attn = epipolar_Attention_Block(num_heads = 4, query_channels=query_channels, \n",
    "                                  key_channels=key_channels, attn_resolution=attn_resolution)\n",
    "\n",
    "\n",
    "res = e_attn(source_input, Unet_input, t, W_Mat)\n",
    "\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test for whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = {'image_size': 64, 'num_channels': 192, \n",
    "        'num_res_blocks': 3, 'num_heads': 4, 'num_heads_upsample': -1, \n",
    "        'num_head_channels': 64, 'attention_resolutions': '32,16,8', 'channel_mult': '', 'dropout': 0.1, \n",
    "        'class_cond': True, 'use_checkpoint': False, 'use_scale_shift_norm': True, \n",
    "        'resblock_updown': True, 'use_fp16': False, 'use_new_attention_order': True, \n",
    "        'learn_sigma': True, 'diffusion_steps': 1000, 'noise_schedule': 'cosine', \n",
    "        'timestep_respacing': '250', 'use_kl': False, 'predict_xstart': False, 'rescale_timesteps': False, \n",
    "        'rescale_learned_sigmas': False, 'feature_channels': 1, 'epipolar_distance_threshold': 0.1}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = create_epipolar_model(**keys)\n",
    "\n",
    "    \n",
    "model.load_state_dict(torch.load('weight/pretrain/openai/64x64_diffusion.pt'), strict=False)\n",
    "model = model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.to(device)\n",
    "    \n",
    "x = torch.randn(1, 3, 64, 64)\n",
    "t = torch.ones(1, dtype=torch.float32) * 1000\n",
    "weight_Mat = torch.randn(1, 64*64, 64*64)\n",
    "f = torch.randn(1, 1, 64, 64)\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "#    \n",
    "#    print(f\"Parameter '{name}' is on device: {param.device}\")\n",
    "\n",
    "\n",
    "x = x.to(device)\n",
    "t = t.to(device)\n",
    "weight_Mat = weight_Mat.to(device)\n",
    "f = f.to(device)\n",
    "\n",
    "res = model(x, t, f, weight_Mat)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\zero123\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.script_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mepipolar_unet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscript_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --dropout 0.1 --image_size 64 --learn_sigma True --noise_schedule cosine --num_channels 192 \u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils.script_utils'"
     ]
    }
   ],
   "source": [
    "from models.epipolar_unet import *\n",
    "\n",
    "from models.unet import *\n",
    "from utils.script_util import *\n",
    "from dataset.dataset import *\n",
    "\n",
    "#MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --dropout 0.1 --image_size 64 --learn_sigma True --noise_schedule cosine --num_channels 192 \n",
    "from dataset.dataset import *\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "test_ds = RealEstate_dataset(info_root = 'dataset/RealEstate10K', data_root = 'dataset/RealEstate_data', mode = 'test')\n",
    "loader = DataLoader(test_ds, batch_size=1, num_workers=0, pin_memory=True,\n",
    "                                drop_last=True)\n",
    "# for idx, data in enumerate(loader):\n",
    "#     print(idx, end='\\r')\n",
    "    \n",
    "# prepare source view encoder\n",
    "model_type = \"DPT_Large\" \n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform\n",
    "    \n",
    "# fetch data\n",
    "example = None\n",
    "for idx, data in enumerate(loader):\n",
    "    example = data\n",
    "    break\n",
    "\n",
    "img_s, img_t, K_s, R_rel, t_rel = example\n",
    "\n",
    "# print(torch.min(img_s))\n",
    "img_s = img_s.cuda()\n",
    "# compute depth\n",
    "feature = midas(img_s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.epipolar_unet import *\n",
    "\n",
    "from models.unet import *\n",
    "from utils.script_util import *\n",
    "\n",
    "from dataset.dataset import *\n",
    "\n",
    "#MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --dropout 0.1 --image_size 64 --learn_sigma True --noise_schedule cosine --num_channels 192 \n",
    "from dataset.dataset import *\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# print(feature.shape)from utils.script_utils import *\n",
    "from utils.resample import *\n",
    "\n",
    "keys = {'image_size': 64, 'num_channels': 192, \n",
    "        'num_res_blocks': 3, 'num_heads': 4, 'num_heads_upsample': -1, \n",
    "        'num_head_channels': 64, 'attention_resolutions': '32,16,8', 'channel_mult': '', 'dropout': 0.1, \n",
    "        'class_cond': True, 'use_checkpoint': False, 'use_scale_shift_norm': True, \n",
    "        'resblock_updown': True, 'use_fp16': True, 'use_new_attention_order': True, \n",
    "        'learn_sigma': True, 'diffusion_steps': 1000, 'noise_schedule': 'cosine',\n",
    "        'timestep_respacing': '250', 'use_kl': False, 'predict_xstart': False, 'rescale_timesteps': False, \n",
    "        'rescale_learned_sigmas': False, 'use_epipolar': True, 'schedule_sampler': 'uniform', 'num_epochs': 1000}\n",
    "    \n",
    "model, diffusion = create_model_and_diffusion(**keys)\n",
    "schedule_sampler = create_named_schedule_sampler(keys['schedule_sampler'], diffusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\zero123\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils.dataset import *\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_ds = RealEstate_dataset(info_root = 'dataset/RealEstate10K', data_root = 'dataset/RealEstate_data', mode = 'test')\n",
    "loader = DataLoader(test_ds, batch_size=2, shuffle=False, num_workers=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = None\n",
    "for idx, data in enumerate(loader):\n",
    "    example = data \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "img_s, img_t, K_s, R_rel, t_rel = example\n",
    "\n",
    "print(K_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.epipolar_unet import *\n",
    "from models.epipolar_unet import *\n",
    "\n",
    "from models.unet import *\n",
    "# from utils.script_util import *\n",
    "\n",
    "#MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --dropout 0.1 --image_size 64 --learn_sigma True --noise_schedule cosine --num_channels 192 \n",
    "from utils.dataset import *\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.logger as logger\n",
    "# from utils.train_util import *\n",
    "from utils.script_util import *\n",
    "\n",
    "# print(feature.shape)from utils.script_utils import *\n",
    "from utils.resample import *\n",
    "\n",
    "keys = {'image_size': 64, 'num_channels': 192, \n",
    "        'num_res_blocks': 3, 'num_heads': 4, 'num_heads_upsample': -1, \n",
    "        'num_head_channels': 64, 'attention_resolutions': '32,16,8', 'channel_mult': '', 'dropout': 0.1, \n",
    "        'class_cond': True, 'use_checkpoint': False, 'use_scale_shift_norm': True, \n",
    "        'resblock_updown': True, 'use_fp16': False, 'use_new_attention_order': True, \n",
    "        'learn_sigma': True, 'diffusion_steps': 1000, 'noise_schedule': 'cosine',\n",
    "        'timestep_respacing': '250', 'use_kl': False, 'predict_xstart': False, 'rescale_timesteps': False, \n",
    "        'rescale_learned_sigmas': False, 'use_epipolar': True}\n",
    "\n",
    "sample_args = {'schedule_sampler': 'uniform', 'num_epochs': 1000}\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(**keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = torch.randn(1, 1, 64, 64).cuda()\n",
    "W_Mat = torch.randn(1, 64*64, 64*64).cuda()\n",
    "x = torch.randn(1, 3, 64, 64).cuda()\n",
    "t = torch.ones(1, dtype=torch.float32) * 1000\n",
    "t = t.cuda()\n",
    "model = model.cuda()\n",
    "model_kwargs = {'f' : feature, 'Weight_Mat': W_Mat}\n",
    "\n",
    "res = model(x, t, **model_kwargs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('zero123')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7fef7d32d27d7a6c28eebb43bd4c4a58af0db382635305f69da271da1e7bd3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
